{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=10):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        print(f\"IMAGE_SIZE----{self.img_size}\")\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        print(f\"PATCH SIZE------{self.patch_size}\")\n",
    "\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        print(F\"N_PARAMETERS----{self.n_patches}\")\n",
    "\n",
    "\n",
    "        # Check for valid patch size\n",
    "        if img_size % patch_size != 0:\n",
    "            raise ValueError(\"Image size must be divisible by patch size.\")\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        print(f\"PROJECT LAYER----- {self.proj}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Get the input shape\n",
    "        print(f\"FORWARD LAYER---- {H, W}\")\n",
    "        \n",
    "        # Check for valid input dimensions\n",
    "        if H != self.img_size or W != self.img_size:\n",
    "            raise ValueError(f\"Input image size ({H}x{W}) doesn't match model's img_size ({self.img_size}).\")\n",
    "\n",
    "        x = self.proj(x)  # (B, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        print(f\"PROJECTION AFTER FORWARD PASS {x.shape}\")\n",
    "        x = x.flatten(2)\n",
    "        print(f\"AFTER FLATTENING {x.shape}\")\n",
    "        x= x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        print(f\"AFTER TRANSPOSE {x.shape}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_patches ,embed_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # learnable postional embedding with class token\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1,n_patches+1,embed_dim))\n",
    "        print(f\"postional embedding {self.positional_embedding.shape}\")\n",
    "\n",
    "        # learnable class token\n",
    "        self.class_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        print(f\"class token {self.class_token.shape}\")\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        print(f\"Batch Size {batch_size}\")\n",
    "        cls_token = self.class_token.expand(batch_size,-1 ,-1)\n",
    "        print(f\"Class token {cls_token.shape}\")\n",
    "\n",
    "        # concate class token \n",
    "        x = torch.cat([cls_token,x],dim = 1)\n",
    "        print(f\"Concatinate class roken {x.shape}\")\n",
    "        x = x + self.positional_embedding\n",
    "        print(x.shape)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Patch Embedding and Positional Encoding\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, \n",
    "                 in_chans=3, embed_dim=10,):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch Embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, \n",
    "            patch_size=patch_size, \n",
    "            in_chans=in_chans, \n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            n_patches=(img_size//patch_size)**2, \n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Further processing would happen here\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_SIZE----224\n",
      "PATCH SIZE------4\n",
      "N_PARAMETERS----3136\n",
      "PROJECT LAYER----- Conv2d(3, 10, kernel_size=(4, 4), stride=(4, 4))\n",
      "postional embedding torch.Size([1, 3137, 10])\n",
      "class token torch.Size([1, 1, 10])\n",
      "FORWARD LAYER---- (224, 224)\n",
      "PROJECTION AFTER FORWARD PASS torch.Size([4, 10, 56, 56])\n",
      "AFTER FLATTENING torch.Size([4, 10, 3136])\n",
      "AFTER TRANSPOSE torch.Size([4, 3136, 10])\n",
      "\n",
      "\n",
      "\n",
      "Batch Size 4\n",
      "Class token torch.Size([4, 1, 10])\n",
      "Concatinate class roken torch.Size([4, 3137, 10])\n",
      "torch.Size([4, 3137, 10])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "img_size = 224\n",
    "patch_size = 4\n",
    "embed_dim = 10\n",
    "\n",
    "# Create a random input image tensor\n",
    "input_image = torch.randn(4, 3, img_size, img_size)  # 4 images in batch\n",
    "\n",
    "# Initialize Vision Transformer\n",
    "vit = VisionTransformer(\n",
    "    img_size=img_size, \n",
    "    patch_size=patch_size, \n",
    "    embed_dim=embed_dim\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = vit(input_image)\n",
    "\n",
    "# print(\"Input Image Shape:\", input_image.shape)\n",
    "# print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATCH SIZE tensor([[[ 1.0334, -0.6626,  0.2378],\n",
      "         [ 0.0790, -0.3910,  0.2376],\n",
      "         [ 0.6799,  1.2311, -0.0737],\n",
      "         [ 0.5852,  0.8770,  1.2304]],\n",
      "\n",
      "        [[-1.2126, -2.8163,  1.8042],\n",
      "         [ 0.7125,  0.4960,  0.5178],\n",
      "         [ 0.2315,  2.6094, -0.2609],\n",
      "         [-0.0798,  1.3035,  0.0351]]]) \n",
      "\n",
      "POSITIONAL EMBEDDINGS Parameter containing:\n",
      "tensor([[[ 0.6438, -1.9903, -0.3504],\n",
      "         [ 0.0607,  0.7025,  0.7990],\n",
      "         [-0.5644,  0.1264, -0.1117],\n",
      "         [ 0.2809, -1.9468,  0.0443],\n",
      "         [ 0.8796,  1.5137, -0.0539]]], requires_grad=True) \n",
      "\n",
      "CLASS TOKEN Parameter containing:\n",
      "tensor([[[ 1.0111,  0.3674, -0.5016]]], requires_grad=True) \n",
      "\n",
      "BATCH SIZE 2 \n",
      "\n",
      "CLASS TOKEN tensor([[[ 1.0111,  0.3674, -0.5016]],\n",
      "\n",
      "        [[ 1.0111,  0.3674, -0.5016]]], grad_fn=<ExpandBackward0>) \n",
      "\n",
      "CONCAT USING TORCH tensor([[[ 1.0111,  0.3674, -0.5016],\n",
      "         [ 1.0334, -0.6626,  0.2378],\n",
      "         [ 0.0790, -0.3910,  0.2376],\n",
      "         [ 0.6799,  1.2311, -0.0737],\n",
      "         [ 0.5852,  0.8770,  1.2304]],\n",
      "\n",
      "        [[ 1.0111,  0.3674, -0.5016],\n",
      "         [-1.2126, -2.8163,  1.8042],\n",
      "         [ 0.7125,  0.4960,  0.5178],\n",
      "         [ 0.2315,  2.6094, -0.2609],\n",
      "         [-0.0798,  1.3035,  0.0351]]], grad_fn=<CatBackward0>)\n",
      "ENCODED X tensor([[[ 1.6549, -1.6229, -0.8520],\n",
      "         [ 1.0941,  0.0399,  1.0368],\n",
      "         [-0.4854, -0.2646,  0.1259],\n",
      "         [ 0.9607, -0.7157, -0.0295],\n",
      "         [ 1.4648,  2.3907,  1.1766]],\n",
      "\n",
      "        [[ 1.6549, -1.6229, -0.8520],\n",
      "         [-1.1520, -2.1138,  2.6032],\n",
      "         [ 0.1482,  0.6225,  0.4061],\n",
      "         [ 0.5123,  0.6625, -0.2166],\n",
      "         [ 0.7998,  2.8171, -0.0188]]], grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimplePositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_tokens, embed_dim):\n",
    "        super().__init__()\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, n_tokens + 1, embed_dim)) # +1 for class token\n",
    "        print(f\"POSITIONAL EMBEDDINGS {self.positional_embeddings} \\n\")\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        print(f\"CLASS TOKEN {self.class_token} \\n\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        print(f\"BATCH SIZE {batch_size} \\n\")\n",
    "        cls_token = self.class_token.expand(batch_size, -1, -1)  # Expand class token for the batch\n",
    "        print(f\"CLASS TOKEN {cls_token} \\n\")\n",
    "        x = torch.cat([cls_token, x], dim=1)  # Concatenate class token\n",
    "        print(f\"CONCAT USING TORCH {x}\")\n",
    "        return x + self.positional_embeddings\n",
    "\n",
    "# Example usage:\n",
    "n_tokens = 4  # Example: 4 patches\n",
    "embed_dim = 3  # Example embedding dimension\n",
    "x = torch.randn(2, n_tokens, embed_dim)  # Batch size of 2\n",
    "print(f\"PATCH SIZE {x} \\n\")\n",
    "pos_enc = SimplePositionalEncoding(n_tokens, embed_dim)\n",
    "encoded_x = pos_enc(x)\n",
    "print(f\"ENCODED X {encoded_x} \\n\")\n",
    "print(encoded_x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
